机器学习项目的完整流程
有监督学习和无监督学习区别
线性分类器和非线性分类器区别、优劣
对不同维度的特征应如何选择线性分类器或非线性分类器
什么是生成模型和判别模型
常见生成式模型和判别式模型有哪些
置信区间的定义
过拟合解释
正则化解释
L1正则化和L2正则化
为什么L1正则化相较于L2正则化会产生更加稀疏的矩阵
* L1和L2各有什么应用场景
正则化为什么能防止过拟合
最优化方法有哪些
简述梯度下降算法
如何处理样本不均衡问题
如何评估分类器
简述ROC和AUC
ROC的优劣
什么是交叉验证
交叉验证的几种方法
* 损失函数有什么作用
* 损失函数有哪些形式
什么是特征工程
对于一个新的项目如何生成模型所需要使用的特征
原始数据通常存在哪些问题
数据抽样方法有哪些
归一化为什么能提高梯度下降法求解最优解的速度
归一化在什么情况下可以提高精度
哪些机器学习算法不需要对特征归一化
树形结构为什么不需要归一化
什么是异常值，如何检测异常值
如何处理异常值
如何处理原始数据的缺失值
什么是特征选择，为什么要特征选择
特征选择方法有哪些
如何使用卡方检验进行特征选择
特征提取是什么，与特征选择区别
PCA工作原理
如何确定PCA的第一主成分、第二主成分
PCA优缺点

简述决策树原理
简述决策树构建过程
信息增益率有什么优缺点
如何对决策树剪枝
C4.5对ID3做了哪些改进
C4.5如何处理连续数值型属性
C4.5与CART区别
简述分类树与回归树
CART如何生成回归树
CART对离散特征取值数目大于两种的如何处理
决策树对缺失值如何处理
决策树属性用完但仍未对决策树完成划分应怎么办
如何避免决策树过拟合
二叉决策树和多分支决策树区别
在决策树构建过程中哪个步骤最耗时
决策树在所有类型的数据上的表现都不错？
决策树中的特征会被重复选择吗
决策树优缺点

logistic回归推导
简述线性回归
为什么logistic回归可以使用最大似然函数作为损失函数
logistic回归与线性回归的联系和区别
logistic的样本应满足什么分布
logistic输出的是真实的概率吗
logistic会发生过拟合吗，如何解决
什么是特征离散化和特征交叉
logistic为什么要对特征进行离散化
logistic为什么常常做特征组合（特征交叉）
logistic如果有很多特征高度相关或有重复的特征，会造成怎样的影响
为什么logistic在训练的过程中将高度相关的特征去掉
logistic最优化过程中如何避免局部极小值
线性回归损失函数为何是2次
logistic特征系数的绝对值可以认为是特征的重要性吗
如何使用logistic实现多分类
logistic参数归一化是否对结果有影响
logistic优缺点

简述朴素贝叶斯算法原理和工作流程
什么是先验概率和后验概率
什么是条件概率
朴素贝叶斯中的朴素是何意思
简述贝叶斯决策理论
朴素贝叶斯算法的前提假设是什么
为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果
朴素贝叶斯可以做多分类吗
什么是朴素贝叶斯中的零概率问题，如何解决
朴素贝叶斯中概率计算的下溢问题如何解决
朴素贝叶斯分类器对异常值敏感吗
当数据的属性是连续型变量时，朴素贝叶斯如何处理
朴素贝叶斯对缺失值敏感吗
朴素贝叶斯有哪几种常用的分类模型
朴素贝叶斯算法中使用拉普拉斯平滑，拉普拉斯因子的大小如何确定
为什么说朴素贝叶斯是高偏差低方差
朴素贝叶斯的增量计算
高度相关的特征对朴素贝叶斯有什么影响
朴素贝叶斯的应用场景有哪些
朴素贝叶斯优缺点
